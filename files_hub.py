# files_hub.py (ìµœì¢… êµì •ë³¸)
import math
import streamlit as st
import pandas as pd
from datetime import datetime
from typing import List, Dict, Tuple

from config import CONFIG
from utils import safe_text, safe_excerpt
from storage_blob import list_blobs_detailed, download_blob
from graph import list_onedrive_root, list_onedrive_children, download_onedrive_file
from docintel import extract_text_naive, extract_text_docintel
from storage_logs import log_activity
from search import ensure_search_ready, make_safe_key, upsert_documents_with_embeddings
from pii import scan_pii
from purview import apply_label_stub
from owners_registry import set_owner, get_owner

# ----------------------------
# ë‚´ë¶€ ìœ í‹¸
# ----------------------------

def go(page_name: str):
    st.session_state["__page"] = page_name
    st.session_state.pop("_nav_to", None)
    st.rerun()

def _is_doc(name: str) -> bool:
    name = (name or "").lower()
    return name.endswith((".pdf", ".txt", ".md", ".docx", ".pptx", ".xlsx"))

def _extract_text(name: str, content: bytes, use_docintel: bool) -> str:
    if use_docintel:
        return extract_text_docintel(content, mime_type="application/octet-stream")
    return extract_text_naive(name, content)

def _paginate(items: List[Dict], page: int, page_size: int) -> Tuple[List[Dict], int]:
    total = len(items)
    start = (page - 1) * page_size
    end = start + page_size
    return items[start:end], math.ceil(total / page_size) if page_size else 1

def _raw_id_of_row(r: Dict) -> str:
    """ì›ë³¸ ID: Blobì´ë©´ ê²½ë¡œ/íŒŒì¼ëª…, OneDriveë©´ ë“œë¼ì´ë¸Œ ì•„ì´í…œ id"""
    return r["id"] if r.get("source") == "onedrive" else r["name"]

def _search_id_of_row(r: Dict) -> str:
    """ì¸ë±ìŠ¤ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì•ˆì „ id"""
    return make_safe_key(_raw_id_of_row(r))

# ----------------------------
# ë°ì´í„° ì†ŒìŠ¤ë³„ ëª©ë¡ ìˆ˜ì§‘ (ìºì‹œ)
# ----------------------------
@st.cache_data(ttl=60)
def _fetch_blob_listing(prefix: str = "") -> List[Dict]:
    rows = list_blobs_detailed(prefix=prefix)
    for r in rows:
        r["source"] = "blob"
        r["id"] = r["name"]        # ì›ë³¸ID = íŒŒì¼ ê²½ë¡œ/ì´ë¦„
        r["is_folder"] = False
    return rows

@st.cache_data(ttl=60)
def _fetch_onedrive_listing(folder_id: str = None) -> List[Dict]:
    items = list_onedrive_children(folder_id) if folder_id else list_onedrive_root()
    rows = []
    for it in items:
        rows.append({
            "id": it.get("id"),     # ì›ë³¸ID = drive item id
            "name": it.get("name"),
            "size": it.get("size"),
            "content_type": "folder" if "folder" in it else it.get("file", {}).get("mimeType"),
            "last_modified": it.get("lastModifiedDateTime"),
            "source": "onedrive",
            "is_folder": "folder" in it
        })
    return rows

# ----------------------------
# ì¼ê´„ ì¸ë±ì‹±/ì—…ì„œíŠ¸
# ----------------------------
def _bulk_index(docs_meta: List[Dict], source: str, use_docintel: bool, batch: int = 10):
    ok_cnt, fail_cnt = 0, 0
    payload_batch = []
    for meta in docs_meta:
        # í´ë”ëŠ” ê±´ë„ˆëœ€
        if meta.get("is_folder"):
            continue

        try:
            if source == "blob":
                data = download_blob(meta["name"])
                name = meta["name"]; original_id = meta["name"]
                path = meta["name"]
            else:
                data = download_onedrive_file(meta["id"])
                name = meta["name"]; original_id = meta["id"]
                path = meta["id"]   # OneDriveëŠ” idë¥¼ path ëŒ€ìš©ìœ¼ë¡œ ë³´ì¡´

            text = _extract_text(name, data, use_docintel=use_docintel)

            payload_batch.append({
                # idëŠ” ì›ë³¸IDë¥¼ ì§‘ì–´ë„£ê³ , upsert_documents ë‚´ë¶€ì—ì„œ ì•ˆì „í‚¤ë¡œ ë³€í™˜ë¨
                "id": original_id,
                "originalId": original_id,
                "name": name,
                "source": source,
                "path": path,
                "content": text,
                "lastModified": meta.get("last_modified") or datetime.utcnow().isoformat(),
                "views": 0,
            })

            if len(payload_batch) >= batch:
                pii = scan_pii(text)
                if pii and sum(len(v) for v in pii.values()) > 0:
                    # ì„ì‹œ: PII ë°œê²¬ ì‹œ Confidential ë¼ë²¨ ìŠ¤í…
                    try:
                        apply_label_stub(original_id, "Confidential")
                    except Exception:
                        pass
                upsert_documents_with_embeddings(payload_batch)
                ok_cnt += len(payload_batch)
                payload_batch.clear()
        except Exception as e:
            fail_cnt += 1
            log_activity(
                st.session_state.get("graph_user_mail","default"),
                "Index",
                "ERROR",
                f"bulk index fail: {meta.get('name')} Â· {e}"
            )

    if payload_batch:
        pii = scan_pii(text)
        if pii and sum(len(v) for v in pii.values()) > 0:
            # ì„ì‹œ: PII ë°œê²¬ ì‹œ Confidential ë¼ë²¨ ìŠ¤í…
            try:
                apply_label_stub(original_id, "Confidential")
            except Exception:
                pass
        upsert_documents_with_embeddings(payload_batch)
        ok_cnt += len(payload_batch)

    log_activity(
        st.session_state.get("graph_user_mail","default"),
        "Index",
        "INFO",
        f"bulk index done: ok={ok_cnt}, fail={fail_cnt}"
    )
    return ok_cnt, fail_cnt

# ----------------------------
# í–‰ ë‚´ ì•¡ì…˜ ë©”ë‰´(í–„ë²„ê±°) ì²˜ë¦¬
# ----------------------------
def _row_actions(row: Dict, use_docintel: bool, page_tag: str):
    """
    ê° í–‰ ì˜¤ë¥¸ìª½ì˜ 'â‹®' popover ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ëŠ” ë²„íŠ¼ë“¤.
    """
    name = row.get("name")
    source = row.get("source")
    rid = row.get("id")  # ì›ë³¸ID

    def _download_and_extract():
        data = download_blob(name) if source == "blob" else download_onedrive_file(rid)
        text = _extract_text(name, data, use_docintel=use_docintel)
        return data, text

    # ID ê°€ì‹œí™” (ë””ë²„ê¹…/í™•ì¸ìš©)
    original_id = _raw_id_of_row(row)
    index_id = _search_id_of_row(row)
    # st.text_input("originalId", value=original_id, key=f"ori_{page_tag}_{index_id}", disabled=True)
    # st.text_input("index id (search)", value=index_id, key=f"sid_{page_tag}_{index_id}", disabled=True)
    
    cur = get_owner(original_id)
    email_in = st.text_input("ë‹´ë‹¹ì ì´ë©”ì¼", value=cur.get("email",""), key=f"ownermail_{page_tag}_{index_id}")
    phone_in = st.text_input("ë‹´ë‹¹ì íœ´ëŒ€í°", value=cur.get("phone",""), key=f"ownerphone_{page_tag}_{index_id}")
    if st.button("ğŸ’¾ ë‹´ë‹¹ì ì €ì¥", key=f"owner_save_{page_tag}_{index_id}"):
        try:
            saved = set_owner(original_id, email=email_in, phone=phone_in)
            st.success("ë‹´ë‹¹ì ì €ì¥ ì™„ë£Œ")
            st.caption(f"ê²€ì¦: RowKey={saved['_RowKey']} Â· Table={saved['_Table']}")
            st.caption(f"OriginalId={saved['OriginalId']} / Email={saved['Email']} / Phone={saved['Phone']}")
        except Exception as e:
            st.error(f"ì €ì¥ ì‹¤íŒ¨: {e}")        

    # ë¯¸ë¦¬ë³´ê¸°
    if st.button("ğŸ‘ ë¯¸ë¦¬ë³´ê¸°", key=f"pv_{page_tag}_{index_id}"):
        try:
            _, text = _download_and_extract()
            st.code(safe_excerpt(text, 1000))
            log_activity(st.session_state.get("graph_user_mail","default"), "FilesHub", "INFO", f"preview: {name}")
        except Exception as e:
            st.error(f"ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {e}")

    # ë¬¸ì„œ ê°ì‚¬
    if st.button("ğŸ§¾ ë¬¸ì„œ ê°ì‚¬ë¡œ ì´ë™", key=f"audit_{page_tag}_{index_id}"):
        try:
            _, text = _download_and_extract()
            st.session_state["current_doc"] = {"name": name, "id": rid, "text": text}
            log_activity(st.session_state.get("graph_user_mail","default"), "FilesHub", "INFO", f"to Audit: {name}")
            go("ğŸ§¾ ë¬¸ì„œ ê°ì‚¬")   # â† ì—¬ê¸°!
        except Exception as e:
            st.error(f"ê°ì‚¬ ì´ë™ ì‹¤íŒ¨: {e}")

    # ìœ ì‚¬ ê²€ìƒ‰/ë³‘í•©
    if st.button("ğŸ—‚ ìœ ì‚¬ ê²€ìƒ‰/ë³‘í•© ê°€ì´ë“œ", key=f"cur_{page_tag}_{index_id}"):
        try:
            _, text = _download_and_extract()
            st.session_state["current_doc"] = {"name": name, "id": rid, "text": text}
            log_activity(st.session_state.get("graph_user_mail","default"), "FilesHub", "INFO", f"to Curation: {name}")
            go("ğŸ—‚ï¸ ìœ ì‚¬ ê²€ìƒ‰ / ë³‘í•© ê°€ì´ë“œ")  # â† ì—¬ê¸°!
        except Exception as e:
            st.error(f"ê°€ì´ë“œ ì´ë™ ì‹¤íŒ¨: {e}")

    # ì—…ì„œíŠ¸
    if st.button("â¬† ì¸ë±ìŠ¤ ì—…ì„œíŠ¸", key=f"up_{page_tag}_{index_id}"):
        try:
            _, text = _download_and_extract()
            payload = [{
                "id": original_id,            # ì›ë³¸ID â†’ upsert ë‚´ì—ì„œ ì•ˆì „í‚¤ë¡œ ë³€í™˜
                "originalId": original_id,
                "name": name,
                "source": source,
                "path": (row.get("name") if source == "blob" else row.get("id")),
                "content": text,
                "lastModified": row.get("last_modified") or datetime.utcnow().isoformat(),
                "views": 0
            }]
            upsert_documents_with_embeddings(payload)
            st.success("ì—…ì„œíŠ¸ ì™„ë£Œ")
            log_activity(st.session_state.get("graph_user_mail","default"), "Search", "INFO", f"upsert: {name}")
        except Exception as e:
            st.error(f"ì—…ì„œíŠ¸ ì‹¤íŒ¨: {e}")
            log_activity(st.session_state.get("graph_user_mail","default"), "Search", "ERROR", f"upsert fail: {name} Â· {e}")

# ----------------------------
# ë©”ì¸ ë Œë”
# ----------------------------
def render_files_hub():
    # 1) Search ì¸ë±ìŠ¤ ë³´ì¥
    try:
        ensure_search_ready(create_if_missing=True)
    except Exception as e:
        st.error(f"Search ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return

    st.title("ğŸ“ DocSpace")
    st.caption("íŒŒì¼ ëª©ë¡ì„ í‘œì‹œí•˜ê³ , ê° í–‰ì˜ í–„ë²„ê±°(â‹®) ë©”ë‰´ì—ì„œ ë°”ë¡œ ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")

    source_default = CONFIG.get("STORAGE_MODE", "onedrive")
    source = st.radio("ë°ì´í„° ì†ŒìŠ¤", ["blob", "onedrive"], index=0 if source_default == "blob" else 1, horizontal=True)

    use_docintel = st.toggle("Azure Document Intelligence OCR ì‚¬ìš©(ê°€ëŠ¥ì‹œ)", value=False)

    # q = st.text_input("ì´ë¦„ í•„í„°", value="")
    # if q:
    #     q_low = q.lower()
    #     rows = [r for r in rows if q_low in (r.get("name","").lower())]

    # 2) ëª©ë¡ ìë™ ë¡œë”©
    with st.spinner("íŒŒì¼ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘â€¦"):
        if source == "blob":
            rows = _fetch_blob_listing()
        else:
            rows = _fetch_onedrive_listing()

    # í´ë” ì œì™¸(í‘œì—ì„œ ë…¸ì¶œ ë§‰ê¸°)
    rows = [r for r in rows if not r.get("is_folder")]

    # ë¬¸ì„œí˜•ë§Œ ë³´ê¸°
    if st.checkbox("ë¬¸ì„œ í™•ì¥ìë§Œ ë³´ê¸° (.pdf/.txt/.md/.docx/.pptx/.xlsx)", value=True):
        rows = [r for r in rows if _is_doc(r.get("name",""))]

    # 3) ìµœì´ˆ ì¼ê´„ ì¸ë±ì‹±
    if not st.session_state.get("_indexed_once"):
        st.info("ì•„ì§ ì „ì²´ íŒŒì¼ ì¸ë±ì‹±ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ í˜„ì¬ ëª©ë¡(í•„í„° ê²°ê³¼)ì„ ì¼ê´„ ì—…ì„œíŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    c1, c2 = st.columns([1,3])
    with c1:
        if st.button("ğŸš€ í˜„ì¬ ëª©ë¡ ì¼ê´„ ì¸ë±ì‹±/ì—…ì„œíŠ¸"):
            metas = rows
            ok, fail = _bulk_index(metas, source=source, use_docintel=use_docintel)
            st.session_state["_indexed_once"] = True
            st.success(f"ì¼ê´„ ì¸ë±ì‹± ì™„ë£Œ Â· ì„±ê³µ {ok} Â· ì‹¤íŒ¨ {fail}")

    # ê²€ìƒ‰/í•„í„°
    page_size = st.selectbox("í˜ì´ì§€ í¬ê¸°", [10, 20, 50, 100], index=1)
    page = st.session_state.get("_files_page", 1)

    # í˜ì´ì§€ë„¤ì´ì…˜
    page_total = 1
    if rows:
        if page < 1: page = 1
        subset, page_total = _paginate(rows, page, page_size)
    else:
        subset = []

    # ====== í…Œì´ë¸”(ê°€ìƒ) ë Œë”: í–„ë²„ê±° ë©”ë‰´ í¬í•¨ ======
    if not subset:
        st.warning("í‘œì‹œí•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # í—¤ë”
    st.markdown("""
    <div style="display:grid;grid-template-columns: 1fr 120px 200px 160px 90px;gap:8px;font-weight:600;">
      <div>ì´ë¦„</div><div>í¬ê¸°</div><div>ìˆ˜ì •ì‹œê°</div><div style="text-align:right">ì•¡ì…˜</div>
    </div>
    <hr style="opacity:.2;margin:6px 0 12px 0"/>
    """, unsafe_allow_html=True)

    # í–‰ë“¤
    for r in subset:
        name = r.get("name") or "-"
        size = r.get("size") or "-"
        lmod = r.get("last_modified") or "-"
        page_tag = f"{page}"

        cols = st.columns([6, 2, 3, 1], gap="small")
        with cols[0]:
            st.write(name)
        with cols[1]:
            st.caption(f"{size/1024/1024:.1f} MB" if isinstance(size, (int, float)) else "-")
        with cols[2]:
            st.caption(lmod)
        # í–„ë²„ê±° íŒì˜¤ë²„
        with cols[3]:
            with st.popover("â‹®", use_container_width=True):
                st.markdown(f"**{name}**")
                _row_actions(r, use_docintel=use_docintel, page_tag=page_tag)


    # í˜ì´ì§€ ì»¨íŠ¸ë¡¤
    pc1, pc2, pc3 = st.columns([1,2,1])
    with pc1:
        if st.button("â—€ï¸ ì´ì „"):
            if page > 1:
                st.session_state["_files_page"] = page - 1
                st.rerun()
    with pc2:
        st.caption(f"í˜ì´ì§€ {page} / {page_total} Â· ì´ {len(rows)}ê±´")
    with pc3:
        if st.button("ë‹¤ìŒ â–¶ï¸"):
            if page < page_total:
                st.session_state["_files_page"] = page + 1
                st.rerun()

