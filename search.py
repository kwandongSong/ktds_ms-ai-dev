# search.py
import os, requests, datetime as dt
from typing import List, Dict
from config import CONFIG
import json
import requests
import streamlit as st
import base64, re, requests, json

API_VERSION = "2023-11-01"

def _hdr():
    key = CONFIG.get("SEARCH_API_KEY")
    if not key:
        raise RuntimeError("SEARCH_API_KEY ëˆ„ë½")
    return {"Content-Type":"application/json", "api-key": key}

def _ep():
    ep = (CONFIG.get("SEARCH_ENDPOINT") or "").rstrip("/")
    if not ep.startswith("https://"):
        raise RuntimeError(f"SEARCH_ENDPOINT í˜•ì‹ ì˜¤ë¥˜: {ep}")
    return ep

def _idx():
    idx = CONFIG.get("SEARCH_INDEX")
    if not idx:
        raise RuntimeError("SEARCH_INDEX ëˆ„ë½")
    return idx


SAFE_KEY_RE = re.compile(r"^[A-Za-z0-9_\-=]+$")

def make_safe_key(raw: str) -> str:
    if not raw:
        raise ValueError("empty key")
    if SAFE_KEY_RE.fullmatch(raw):
        return raw
    return base64.urlsafe_b64encode(raw.encode("utf-8")).decode("ascii").rstrip("=")

# ---------- ì¸ë±ìŠ¤ ë³´ì¥ ----------
_CACHED_FIELDS = None

def ensure_search_ready(create_if_missing: bool = True) -> str:
    """
    Search ì„œë¹„ìŠ¤/ì¸ë±ìŠ¤ì— ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸.
    ì—†ìœ¼ë©´ create_if_missing=Trueì¼ ë•Œ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆë¡œ ìƒì„±.
    """
    ep, idx = _ep(), _idx()
    url = f"{ep}/indexes('{idx}')?api-version={API_VERSION}"
    r = requests.get(url, headers=_hdr(), timeout=20)
    if r.status_code == 200:
        return "ready"

    if r.status_code == 404:
        if not create_if_missing:
            raise requests.HTTPError(f"Index not found: {idx}", response=r)
        # ìƒì„± ì‹œë„
        create_index_if_missing()
        return "created"

    # 401/403/400 ë“±ì€ ê·¸ëŒ€ë¡œ ë³´ì—¬ì£¼ì–´ ë””ë²„ê¹…
    try:
        detail = r.json()
    except Exception:
        detail = r.text
    raise requests.HTTPError(f"Search index check failed {r.status_code}: {detail}", response=r)

def get_index_schema_fields():
    """í˜„ì¬ ì¸ë±ìŠ¤ì˜ í•„ë“œ ì´ë¦„ set (ìºì‹œ)"""
    global _CACHED_FIELDS
    if _CACHED_FIELDS is not None:
        return _CACHED_FIELDS

    # ì—†ìœ¼ë©´ ìƒì„±ê¹Œì§€ í¬í•¨í•´ ë³´ì¥
    ensure_search_ready(create_if_missing=True)

    ep, idx = _ep(), _idx()
    url = f"{ep}/indexes('{idx}')?api-version={API_VERSION}"
    r = requests.get(url, headers=_hdr(), timeout=20)
    r.raise_for_status()
    data = r.json()
    _CACHED_FIELDS = {f["name"] for f in data.get("fields", [])}
    return _CACHED_FIELDS




def _base_url():
    ep = CONFIG["SEARCH_ENDPOINT"].rstrip("/")
    idx = CONFIG["SEARCH_INDEX"]
    return f"{ep}/indexes('{idx}')/docs"

def get_index_doc_count() -> int:
    """
    ì¸ë±ìŠ¤ì˜ ì „ì²´ ë¬¸ì„œ ìˆ˜
    """
    url = f"{_base_url()}?api-version={API_VERSION}&search=*&$count=true&$top=0"
    r = requests.get(url, headers=_hdr(), timeout=30)
    r.raise_for_status()
    # $count=trueì¼ ë•Œ, countëŠ” í—¤ë”ê°€ ì•„ë‹ˆë¼ ë³¸ë¬¸ '@odata.count'ì— ë“¤ì–´ì˜´
    data = r.json()
    return int(data.get("@odata.count", 0))

def get_recent_documents(top: int = 20) -> List[Dict]:
    """
    ìµœê·¼ ìˆ˜ì • ë¬¸ì„œ ìƒìœ„ Nê°œ (lastModified í•„ë“œ ê¸°ì¤€)
    ì¸ë±ìŠ¤ì— lastModified(Edm.String or DateTimeOffset) í•„ë“œê°€ ìˆì–´ì•¼ í•¨.
    """
    url = f"{_base_url()}?api-version={API_VERSION}&search=*&$top={top}&$orderby=lastModified desc"
    r = requests.get(url, headers=_hdr(), timeout=30)
    r.raise_for_status()
    hits = r.json().get("value", [])
    # í‘œì¤€í™” ì»¬ëŸ¼
    rows = []
    for h in hits:
        rows.append({
            "id": h.get("id") or h.get("@search.action") or "",
            "name": h.get("name") or h.get("fileName") or "",
            "lastModified": h.get("lastModified"),
            "views": h.get("views", 0),
        })
    return rows

def get_timeseries_counts(days: int = 12) -> List[Dict]:
    """
    ìµœê·¼ Nì¼ ì¼ìë³„ ë¬¸ì„œ ê±´ìˆ˜(ê°„ë‹¨ ì§‘ê³„).
    facet intervalì´ ì–´ë ¤ìš°ë©´ í´ë¼ì´ì–¸íŠ¸ì—ì„œ ìƒìœ„ ë¬¸ì„œë¥¼ ë‚´ë ¤ë°›ì•„ day ë‹¨ìœ„ë¡œ ê·¸ë£¹í•‘.
    """
    # ë„‰ë„‰íˆ ìƒìœ„ 1000ê±´ë§Œ ëŒì–´ì™€ì„œ ì§‘ê³„
    url = f"{_base_url()}?api-version={API_VERSION}&search=*&$top=1000&$orderby=lastModified desc&$select=id,lastModified"
    r = requests.get(url, headers=_hdr(), timeout=30)
    r.raise_for_status()
    vals = r.json().get("value", [])
    # day bucket
    buckets = {}
    today = dt.datetime.utcnow().date()
    start = today - dt.timedelta(days=days-1)

    for v in vals:
        lm = v.get("lastModified")
        if not lm:
            continue
        try:
            # ISO -> date
            day = dt.date.fromisoformat(lm[:10])
        except Exception:
            # ë¬¸ìì—´ì¸ ê²½ìš° 'YYYY-MM-DD' ì• 10ìë§Œ íŒŒì‹±
            try:
                day = dt.datetime.strptime(lm[:10], "%Y-%m-%d").date()
            except Exception:
                continue
        if day < start:
            continue
        buckets[day] = buckets.get(day, 0) + 1

    # ëˆ„ë½ì¼ 0 ì±„ìš°ê¸°
    out = []
    for i in range(days):
        d = start + dt.timedelta(days=i)
        out.append({"date": d.isoformat(), "docs": buckets.get(d, 0)})
    return out



# --- Cognitive Search ì¸ë±ìŠ¤ ê´€ë¦¬/ì—…ì„œíŠ¸/ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥ ë³µì› ---
def create_index_if_missing():
    """ê¸°ë³¸ ìŠ¤í‚¤ë§ˆë¡œ ì¸ë±ìŠ¤ ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ no-op)"""
    ep, idx = _ep(), _idx()
    # ì¡´ì¬ì—¬ë¶€ í™•ì¸
    url_get = f"{ep}/indexes('{idx}')?api-version={API_VERSION}"
    r = requests.get(url_get, headers=_hdr(), timeout=20)
    if r.status_code == 200:
        return "already exists"

    # ìƒì„±
    url_put = f"{ep}/indexes/{idx}?api-version={API_VERSION}"
    payload = {
        "name": idx,
        "fields": [
            {"name":"id","type":"Edm.String","key":True,"filterable":True},
            {"name":"originalId","type":"Edm.String","filterable":True},
            {"name":"name","type":"Edm.String","searchable":True,"sortable":True},
            {"name":"source","type":"Edm.String","filterable":True},
            {"name":"path","type":"Edm.String","filterable":True},
            {"name":"content","type":"Edm.String","searchable":True},
            {"name":"lastModified","type":"Edm.String","filterable":True,"sortable":True},
            {"name":"views","type":"Edm.Int32","filterable":True,"sortable":True}
        ]
    }
    r = requests.put(url_put, headers=_hdr(), data=json.dumps(payload), timeout=30)
    r.raise_for_status()
    # ìƒˆ ìŠ¤í‚¤ë§ˆ ìºì‹œ
    global _CACHED_FIELDS
    _CACHED_FIELDS = {f["name"] for f in payload["fields"]}
    return "created"

# ---------- ì—…ì„œíŠ¸ ----------
def upsert_documents(docs, allow_unsafe_keys=False):
    """
    docs: [{id, name, content, lastModified, views, source?, path?, originalId? ...}]
    - í˜„ì¬ ì¸ë±ìŠ¤ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ (ì—†ìœ¼ë©´ ìƒì„±)
    - ì¡´ì¬í•˜ëŠ” í•„ë“œë§Œ ì „ì†¡
    - idëŠ” ì•ˆì „í‚¤ ë³€í™˜, originalIdì— ì›ë³¸ id ë³´ì¡´(ìŠ¤í‚¤ë§ˆ ìˆì„ ë•Œ)
    """
    # ìŠ¤í‚¤ë§ˆ í™•ë³´(í•„ë“œ ìºì‹œ)
    field_names = get_index_schema_fields()

    ep, idx = _ep(), _idx()
    url = f"{ep}/indexes('{idx}')/docs/index?api-version={API_VERSION}"
    if allow_unsafe_keys:
        url += "&allowUnsafeKeys=true"

    value = []
    for d in docs:
        original = d.get("id") or d.get("name")
        safe_id = make_safe_key(original)

        filtered = {k: v for k, v in d.items() if k in field_names and k != "id"}
        filtered["id"] = safe_id
        if "originalId" in field_names:
            filtered.setdefault("originalId", original)

        value.append({"@search.action": "mergeOrUpload", **filtered})

    r = requests.post(url, headers=_hdr(), json={"value": value}, timeout=60)
    if r.status_code >= 400:
        try:
            detail = r.json()
        except Exception:
            detail = r.text
        raise requests.HTTPError(f"Search upsert failed {r.status_code}: {detail}", response=r)
    return r.json()


def vector_search(query_text: str, k: int = 5):
    """
    ë‹¨ìˆœ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ (Text ê¸°ë°˜)
    """
    ep = CONFIG["SEARCH_ENDPOINT"].rstrip("/")
    idx = CONFIG["SEARCH_INDEX"]
    key = CONFIG["SEARCH_API_KEY"]

    url = f"{ep}/indexes('{idx}')/docs/search?api-version=2023-11-01"
    headers = {"Content-Type": "application/json", "api-key": key}
    payload = {
        "search": query_text,
        "queryType": "simple",
        "top": k,
        "select": "id,name,lastModified,views",
    }
    r = requests.post(url, headers=headers, json=payload)
    if r.status_code != 200:
        st.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {r.text}")
        return []
    data = r.json().get("value", [])
    return [{"id": d.get("id"), "name": d.get("name"), "score": d.get("@search.score")} for d in data]


def show_search_guidance(st_container=None):
    """
    Search ì¸ë±ìŠ¤ ë° ë²¡í„° ê²€ìƒ‰ ê°€ì´ë“œ í‘œì‹œìš© (Streamlit UI)
    """
    c = st_container or st
    c.markdown("### ğŸ” Cognitive Search ê°€ì´ë“œ")
    c.markdown("""
    **DocSpace AI**ëŠ” Azure Cognitive Searchë¥¼ ì´ìš©í•´ ë¬¸ì„œë¥¼ ì¸ë±ì‹±í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.  

    **ì£¼ìš” í•¨ìˆ˜**
    - `create_index_if_missing()` : ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸ ë° ìë™ ìƒì„±
    - `upsert_documents(docs)` : ë¬¸ì„œ ì¸ë±ìŠ¤ì— ì—…ì„œíŠ¸
    - `vector_search(query_text, k)` : ë²¡í„°/í…ìŠ¤íŠ¸ ê²€ìƒ‰

    ì¸ë±ìŠ¤ í•„ë“œ ì˜ˆì‹œ:
    ```
    id (key) | name | content | lastModified | views
    ```
    """)

def get_document_by_id(doc_id: str) -> dict:
    """
    ì¸ë±ìŠ¤ì—ì„œ íŠ¹ì • idì˜ ë¬¸ì„œ ë‹¨ê±´ ì¡°íšŒ (content í¬í•¨)
    """
    ep = CONFIG["SEARCH_ENDPOINT"].rstrip("/")
    idx = CONFIG["SEARCH_INDEX"]
    url = f"{ep}/indexes('{idx}')/docs?api-version={API_VERSION}&$filter=id eq '{doc_id}'&$top=1"
    r = requests.get(url, headers=_hdr(), timeout=30)
    r.raise_for_status()
    vals = r.json().get("value", [])
    return vals[0] if vals else {}

def vector_search_by_text(text: str, k: int = 5, select: str = "id,name,lastModified,views") -> List[Dict]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì¿¼ë¦¬í•´ ìƒìœ„ kê°œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ë°˜í™˜
    (ë²¡í„°/í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì„± ì „ PoCìš© simple search)
    """
    ep = CONFIG["SEARCH_ENDPOINT"].rstrip("/")
    idx = CONFIG["SEARCH_INDEX"]
    url = f"{ep}/indexes('{idx}')/docs/search?api-version={API_VERSION}"
    payload = {"search": text[:3000], "queryType": "simple", "top": k, "select": select}
    r = requests.post(url, headers=_hdr(), json=payload, timeout=30)
    r.raise_for_status()
    vals = r.json().get("value", [])
    # ì •ê·œí™”
    out = []
    for v in vals:
        out.append({
            "id": v.get("id"),
            "name": v.get("name"),
            "lastModified": v.get("lastModified"),
            "views": v.get("views"),
            "score": v.get("@search.score"),
        })
    return out
