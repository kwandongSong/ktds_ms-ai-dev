# search.py
import os, requests, datetime as dt
from typing import List, Dict
from config import CONFIG
import json
import requests
import streamlit as st
import base64, re, requests, json
from openai_client import get_embeddings 

API_VERSIONS = [
    "2025-09-01",      # ìµœì‹  ì•ˆì • (ì§€ì› ì‹œ)
    "2024-07-01",      # ë„ë¦¬ ì§€ì›ë˜ëŠ” ì•ˆì •
    "2023-11-01"       # êµ¬ í™˜ê²½ í˜¸í™˜
    "2024-12-01-preview"
]

def _api_url(path: str, api_version: str) -> str:
    ep = _ep()
    return f"{ep}{path}?api-version={api_version}"

def _get_admin_headers():
    h = _hdr()
    # ë°©ì–´ì ìœ¼ë¡œ content-typeì„ ëª…ì‹œ
    if "Content-Type" not in h:
        h["Content-Type"] = "application/json"
    return h

def _try_get(url):
    r = requests.get(url, headers=_get_admin_headers(), timeout=30)
    return r

def _try_put(url, payload):
    r = requests.put(url, headers=_get_admin_headers(), data=json.dumps(payload), timeout=60)
    return r

def _raise_with_text(prefix: str, r: requests.Response):
    try:
        detail = r.json()
    except Exception:
        detail = r.text
    raise requests.HTTPError(f"{prefix} {r.status_code}: {detail}", response=r)

def _hdr():
    key = CONFIG.get("SEARCH_API_KEY")
    if not key:
        raise RuntimeError("SEARCH_API_KEY ëˆ„ë½")
    return {"Content-Type":"application/json", "api-key": key}

def _ep():
    ep = (CONFIG.get("SEARCH_ENDPOINT") or "").rstrip("/")
    if not ep.startswith("https://"):
        raise RuntimeError(f"SEARCH_ENDPOINT í˜•ì‹ ì˜¤ë¥˜: {ep}")
    return ep

def _idx():
    idx = CONFIG.get("SEARCH_INDEX")
    if not idx:
        raise RuntimeError("SEARCH_INDEX ëˆ„ë½")
    return idx


SAFE_KEY_RE = re.compile(r"^[A-Za-z0-9_\-=]+$")

def make_safe_key(raw: str) -> str:
    if not raw:
        raise ValueError("empty key")
    if SAFE_KEY_RE.fullmatch(raw):
        return raw
    return base64.urlsafe_b64encode(raw.encode("utf-8")).decode("ascii").rstrip("=")

_CACHED_FIELDS = None
def ensure_search_ready(create_if_missing=True):
    idx = _idx()

    # 0) ë¨¼ì € ì„œë¹„ìŠ¤ ì‚´ì•„ìˆëŠ”ì§€ /indexes (ëª©ë¡)ë¡œ í™•ì¸
    last_err = None
    chosen_ver = None

    for ver in API_VERSIONS:
        # ëª©ë¡
        r = _try_get(_api_url("/indexes", ver))
        if r.status_code == 200:
            chosen_ver = ver
            break
        last_err = r

    if not chosen_ver:
        _raise_with_text("[ensure_search_ready] list indexes failed for all versions.", last_err)

    # 1) ì¸ë±ìŠ¤ ì¡°íšŒ (ìŠ¬ë˜ì‹œ í¬ë§· â†’ ê´„í˜¸ í¬ë§· ë‘˜ ë‹¤ ì‹œë„)
    for ver in [chosen_ver]:
        # ìŠ¬ë˜ì‹œ í¬ë§·
        r1 = _try_get(_api_url(f"/indexes/{idx}", ver))
        if r1.status_code == 200:
            global API_VERSION
            API_VERSION = ver
            return "ready"
        # ê´„í˜¸ í¬ë§·
        r2 = _try_get(_api_url(f"/indexes('{idx}')", ver))
        if r2.status_code == 200:
            API_VERSION = ver
            return "ready"

        # 404ë©´ ìƒì„± ì‹œë„
        if (r1.status_code == 404 or r2.status_code == 404) and create_if_missing:
            API_VERSION = ver
            return create_index_if_missing()

        last_err = r1 if r1.status_code != 200 else r2

    _raise_with_text("[ensure_search_ready] index check failed.", last_err)

# def ensure_search_ready(create_if_missing: bool = True) -> str:
#     """
#     Search ì„œë¹„ìŠ¤/ì¸ë±ìŠ¤ì— ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸.
#     ì—†ìœ¼ë©´ create_if_missing=Trueì¼ ë•Œ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆë¡œ ìƒì„±.
#     """
#     ep, idx = _ep(), _idx()
#     url = f"{ep}/indexes('{idx}')?api-version={API_VERSION}"
#     r = requests.get(url, headers=_hdr(), timeout=20)
#     if r.status_code == 200:
#         return "ready"

#     if r.status_code == 404:
#         if not create_if_missing:
#             raise requests.HTTPError(f"Index not found: {idx}", response=r)
#         # ìƒì„± ì‹œë„
#         create_index_if_missing()
#         return "created"

#     # 401/403/400 ë“±ì€ ê·¸ëŒ€ë¡œ ë³´ì—¬ì£¼ì–´ ë””ë²„ê¹…
#     try:
#         detail = r.json()
#     except Exception:
#         detail = r.text
#     raise requests.HTTPError(f"Search index check failed {r.status_code}: {detail}", response=r)


def get_index_schema_fields():
    global _CACHED_FIELDS
    if _CACHED_FIELDS:
        return _CACHED_FIELDS

    ensure_search_ready(True)
    ep, idx = _ep(), _idx()

    # ë¨¼ì € ìŠ¬ë˜ì‹œ í¬ë§·
    url1 = _api_url(f"/indexes/{idx}", API_VERSION)
    r = _try_get(url1)
    if r.status_code == 200:
        data = r.json()
        _CACHED_FIELDS = {f["name"] for f in data.get("fields", [])}
        return _CACHED_FIELDS

    # ì‹¤íŒ¨ ì‹œ ê´„í˜¸ í¬ë§·
    url2 = _api_url(f"/indexes('{idx}')", API_VERSION)
    r = _try_get(url2)
    if r.status_code == 200:
        data = r.json()
        _CACHED_FIELDS = {f["name"] for f in data.get("fields", [])}
        return _CACHED_FIELDS

    _raise_with_text("[get_index_schema_fields] failed to fetch schema.", r)

# def get_index_schema_fields():
#     """í˜„ì¬ ì¸ë±ìŠ¤ì˜ í•„ë“œ ì´ë¦„ set (ìºì‹œ)"""
#     global _CACHED_FIELDS
#     if _CACHED_FIELDS is not None:
#         return _CACHED_FIELDS

#     # ì—†ìœ¼ë©´ ìƒì„±ê¹Œì§€ í¬í•¨í•´ ë³´ì¥
#     ensure_search_ready(create_if_missing=True)

#     ep, idx = _ep(), _idx()
#     url = f"{ep}/indexes('{idx}')?api-version={API_VERSION}"
#     r = requests.get(url, headers=_hdr(), timeout=20)
#     r.raise_for_status()
#     data = r.json()
#     _CACHED_FIELDS = {f["name"] for f in data.get("fields", [])}
#     return _CACHED_FIELDS




def _base_url():
    ep = CONFIG["SEARCH_ENDPOINT"].rstrip("/")
    idx = CONFIG["SEARCH_INDEX"]
    return f"{ep}/indexes('{idx}')/docs"

def get_index_doc_count() -> int:
    """
    ì¸ë±ìŠ¤ì˜ ì „ì²´ ë¬¸ì„œ ìˆ˜
    """
    url = f"{_base_url()}?api-version={API_VERSION}&search=*&$count=true&$top=0"
    r = requests.get(url, headers=_hdr(), timeout=30)
    r.raise_for_status()
    # $count=trueì¼ ë•Œ, countëŠ” í—¤ë”ê°€ ì•„ë‹ˆë¼ ë³¸ë¬¸ '@odata.count'ì— ë“¤ì–´ì˜´
    data = r.json()
    return int(data.get("@odata.count", 0))

def get_recent_documents(top: int = 20) -> List[Dict]:
    """
    ìµœê·¼ ìˆ˜ì • ë¬¸ì„œ ìƒìœ„ Nê°œ (lastModified í•„ë“œ ê¸°ì¤€)
    ì¸ë±ìŠ¤ì— lastModified(Edm.String or DateTimeOffset) í•„ë“œê°€ ìˆì–´ì•¼ í•¨.
    """
    url = f"{_base_url()}?api-version={API_VERSION}&search=*&$top={top}&$orderby=lastModified desc"
    r = requests.get(url, headers=_hdr(), timeout=30)
    r.raise_for_status()
    hits = r.json().get("value", [])
    # í‘œì¤€í™” ì»¬ëŸ¼
    rows = []
    for h in hits:
        rows.append({
            "id": h.get("id") or h.get("@search.action") or "",
            "name": h.get("name") or h.get("fileName") or "",
            "lastModified": h.get("lastModified"),
            "views": h.get("views", 0),
        })
    return rows

def get_timeseries_counts(days: int = 12) -> List[Dict]:
    """
    ìµœê·¼ Nì¼ ì¼ìë³„ ë¬¸ì„œ ê±´ìˆ˜(ê°„ë‹¨ ì§‘ê³„).
    facet intervalì´ ì–´ë ¤ìš°ë©´ í´ë¼ì´ì–¸íŠ¸ì—ì„œ ìƒìœ„ ë¬¸ì„œë¥¼ ë‚´ë ¤ë°›ì•„ day ë‹¨ìœ„ë¡œ ê·¸ë£¹í•‘.
    """
    # ë„‰ë„‰íˆ ìƒìœ„ 1000ê±´ë§Œ ëŒì–´ì™€ì„œ ì§‘ê³„
    url = f"{_base_url()}?api-version={API_VERSION}&search=*&$top=1000&$orderby=lastModified desc&$select=id,lastModified"
    r = requests.get(url, headers=_hdr(), timeout=30)
    r.raise_for_status()
    vals = r.json().get("value", [])
    # day bucket
    buckets = {}
    today = dt.datetime.utcnow().date()
    start = today - dt.timedelta(days=days-1)

    for v in vals:
        lm = v.get("lastModified")
        if not lm:
            continue
        try:
            # ISO -> date
            day = dt.date.fromisoformat(lm[:10])
        except Exception:
            # ë¬¸ìì—´ì¸ ê²½ìš° 'YYYY-MM-DD' ì• 10ìë§Œ íŒŒì‹±
            try:
                day = dt.datetime.strptime(lm[:10], "%Y-%m-%d").date()
            except Exception:
                continue
        if day < start:
            continue
        buckets[day] = buckets.get(day, 0) + 1

    # ëˆ„ë½ì¼ 0 ì±„ìš°ê¸°
    out = []
    for i in range(days):
        d = start + dt.timedelta(days=i)
        out.append({"date": d.isoformat(), "docs": buckets.get(d, 0)})
    return out



# --- Cognitive Search ì¸ë±ìŠ¤ ê´€ë¦¬/ì—…ì„œíŠ¸/ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥ ë³µì› ---
def create_index_if_missing():
    idx = _idx()

    # ì„ë² ë”© ì°¨ì›
    try:
        dim = int(CONFIG["AZURE_OPENAI_EMBED_DIM"])
    except Exception:
        raise RuntimeError("CONFIG['AZURE_OPENAI_EMBED_DIM']ë¥¼ ì •ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”. ì˜ˆ: 1536")

    # ìµœì‹  ìŠ¤í‚¤ë§ˆ (2024-07-01 ë° 2025-09-01ì—ì„œ ìœ íš¨)
    payload = {
        "name": idx,
        "fields": [
            {"name":"id","type":"Edm.String","key":True,"filterable":True},
            {"name":"originalId","type":"Edm.String","filterable":True},
            {"name":"name","type":"Edm.String","searchable":True,"sortable":True},
            {"name":"source","type":"Edm.String","filterable":True},
            {"name":"path","type":"Edm.String","filterable":True},
            {"name":"content","type":"Edm.String","searchable":True},
            {
                "name": "contentVector",
                "type": "Collection(Edm.Single)",
                "searchable": True,
                "dimensions": dim,
                # â¬‡ï¸ ë°”ë€ í¬ì¸íŠ¸: í•„ë“œì—ëŠ” vectorSearchProfile ì§€ì •
                "vectorSearchProfile": "vdb-hnsw"
            },
            {"name":"lastModified","type":"Edm.String","filterable":True,"sortable":True},
            {"name":"views","type":"Edm.Int32","filterable":True,"sortable":True}
        ],
        # â¬‡ï¸ ë°”ë€ í¬ì¸íŠ¸: profiles + algorithms êµ¬ì„±
        "vectorSearch": {
            "profiles": [
                {"name": "vdb-hnsw", "algorithm": "hnsw"}
            ],
            "algorithms": [
                {"name": "hnsw", "kind": "hnsw"}
            ]
        }
    }

    # ì¡°íšŒë¡œ ì¡´ì¬ í™•ì¸ (ë²„ì „ì€ ensure_search_readyì—ì„œ ê³¨ë¼ë‘” API_VERSION ì‚¬ìš©)
    url_get = _api_url(f"/indexes('{idx}')", API_VERSION)
    g = _try_get(url_get)
    if g.status_code == 200:
        return "already exists"

    # ìƒì„± (ë‘˜ ë‹¤ ì‹œë„: /indexes/{name} ì™€ /indexes('name'))
    url_put1 = _api_url(f"/indexes/{idx}", API_VERSION)
    r1 = _try_put(url_put1, payload)
    if r1.status_code in (200, 201):
        global _CACHED_FIELDS
        _CACHED_FIELDS = {f["name"] for f in payload["fields"]}
        return "created"

    url_put2 = _api_url(f"/indexes('{idx}')", API_VERSION)
    r2 = _try_put(url_put2, payload)
    if r2.status_code in (200, 201):
        _CACHED_FIELDS = {f["name"] for f in payload["fields"]}
        return "created"

    # ì—ëŸ¬ ìƒì„¸
    try: d1 = r1.json()
    except Exception: d1 = r1.text
    try: d2 = r2.json()
    except Exception: d2 = r2.text
    raise requests.HTTPError(
        f"[create_index_if_missing] PUT failed. slash={r1.status_code}:{d1} | paren={r2.status_code}:{d2}",
        response=r2
    )

# def create_index_if_missing():
#     """ê¸°ë³¸ ìŠ¤í‚¤ë§ˆë¡œ ì¸ë±ìŠ¤ ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ no-op)"""
#     ep, idx = _ep(), _idx()
#     # ì¡´ì¬ì—¬ë¶€ í™•ì¸
#     url_get = f"{ep}/indexes('{idx}')?api-version={API_VERSION}"
#     r = requests.get(url_get, headers=_hdr(), timeout=20)
#     if r.status_code == 200:
#         return "already exists"

#     # ìƒì„±
#     url_put = f"{ep}/indexes/{idx}?api-version={API_VERSION}"
#     payload = {
#         "name": idx,
#         "fields": [
#             {"name":"id","type":"Edm.String","key":True,"filterable":True},
#             {"name":"originalId","type":"Edm.String","filterable":True},
#             {"name":"name","type":"Edm.String","searchable":True,"sortable":True},
#             {"name":"source","type":"Edm.String","filterable":True},
#             {"name":"path","type":"Edm.String","filterable":True},
#             {"name":"content","type":"Edm.String","searchable":True},
#             {"name":"lastModified","type":"Edm.String","filterable":True,"sortable":True},
#             {"name":"views","type":"Edm.Int32","filterable":True,"sortable":True}
#         ]
#     }
#     r = requests.put(url_put, headers=_hdr(), data=json.dumps(payload), timeout=30)
#     r.raise_for_status()
#     # ìƒˆ ìŠ¤í‚¤ë§ˆ ìºì‹œ
#     global _CACHED_FIELDS
#     _CACHED_FIELDS = {f["name"] for f in payload["fields"]}
#     return "created"

# ---------- ì—…ì„œíŠ¸ ----------
# --- ì—…ì„œíŠ¸(í…ìŠ¤íŠ¸ë§Œ) ---
def upsert_documents(docs, allow_unsafe_keys=False):
    ep, idx = _ep(), _idx()
    fields = get_index_schema_fields()
    url = f"{ep}/indexes('{idx}')/docs/index?api-version={API_VERSION}"
    if allow_unsafe_keys: url += "&allowUnsafeKeys=true"

    value = []
    for d in docs:
        original = d.get("id") or d.get("name")
        safe_id = make_safe_key(original)
        filtered = {k:v for k,v in d.items() if k in fields and k!="id"}
        filtered["id"] = safe_id
        if "originalId" in fields: filtered.setdefault("originalId", original)
        value.append({"@search.action":"mergeOrUpload", **filtered})
    r = requests.post(url, headers=_hdr(), json={"value": value}, timeout=60)
    if r.status_code >= 400:
        raise requests.HTTPError(r.text, response=r)
    return r.json()
# def upsert_documents(docs, allow_unsafe_keys=False):
#     """
#     docs: [{id, name, content, lastModified, views, source?, path?, originalId? ...}]
#     - í˜„ì¬ ì¸ë±ìŠ¤ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ (ì—†ìœ¼ë©´ ìƒì„±)
#     - ì¡´ì¬í•˜ëŠ” í•„ë“œë§Œ ì „ì†¡
#     - idëŠ” ì•ˆì „í‚¤ ë³€í™˜, originalIdì— ì›ë³¸ id ë³´ì¡´(ìŠ¤í‚¤ë§ˆ ìˆì„ ë•Œ)
#     """
#     # ìŠ¤í‚¤ë§ˆ í™•ë³´(í•„ë“œ ìºì‹œ)
#     field_names = get_index_schema_fields()

#     ep, idx = _ep(), _idx()
#     url = f"{ep}/indexes('{idx}')/docs/index?api-version={API_VERSION}"
#     if allow_unsafe_keys:
#         url += "&allowUnsafeKeys=true"

#     value = []
#     for d in docs:
#         original = d.get("id") or d.get("name")
#         safe_id = make_safe_key(original)

#         filtered = {k: v for k, v in d.items() if k in field_names and k != "id"}
#         filtered["id"] = safe_id
#         if "originalId" in field_names:
#             filtered.setdefault("originalId", original)

#         value.append({"@search.action": "mergeOrUpload", **filtered})

#     r = requests.post(url, headers=_hdr(), json={"value": value}, timeout=60)
#     if r.status_code >= 400:
#         try:
#             detail = r.json()
#         except Exception:
#             detail = r.text
#         raise requests.HTTPError(f"Search upsert failed {r.status_code}: {detail}", response=r)
#     return r.json()


# --- ì—…ì„œíŠ¸(ì„ë² ë”© í¬í•¨) ---
def upsert_documents_with_embeddings(docs):
    """
    docs: [{id, name, content, ...}]  -> contentVector ì±„ì›Œ ì—…ì„œíŠ¸
    """
    texts = [d.get("content","") for d in docs]
    vectors = get_embeddings(texts)  # ë¦¬ìŠ¤íŠ¸[list[float]] ëª©ë¡
    # ì°¨ì› ê²€ì¦ (ë¡œê·¸/ì˜ˆì™¸)
    expected = int(CONFIG["AZURE_OPENAI_EMBED_DIM"])
    for i, vec in enumerate(vectors):
        if len(vec) != expected:
            raise ValueError(
                f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: expected={expected}, got={len(vec)} (doc idx={i}, id={docs[i].get('id')})"
            )
    enriched = []
    for d, vec in zip(docs, vectors):
        dd = dict(d)
        dd["contentVector"] = vec
        enriched.append(dd)
    return upsert_documents(enriched)

# def vector_search(query_text: str, k: int = 5):
#     """
#     ë‹¨ìˆœ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ (Text ê¸°ë°˜)
#     """
#     ep = CONFIG["SEARCH_ENDPOINT"].rstrip("/")
#     idx = CONFIG["SEARCH_INDEX"]
#     key = CONFIG["SEARCH_API_KEY"]

#     url = f"{ep}/indexes('{idx}')/docs/search?api-version=2023-11-01"
#     headers = {"Content-Type": "application/json", "api-key": key}
#     payload = {
#         "search": query_text,
#         "queryType": "simple",
#         "top": k,
#         "select": "id,name,lastModified,views",
#     }
#     r = requests.post(url, headers=headers, json=payload)
#     if r.status_code != 200:
#         st.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {r.text}")
#         return []
#     data = r.json().get("value", [])
#     return [{"id": d.get("id"), "name": d.get("name"), "score": d.get("@search.score")} for d in data]
# --- ë²¡í„° ê²€ìƒ‰ ---
def vector_search(query_text: str, k: int = 5):
    qvec = get_embeddings([query_text])[0]
    ep, idx = _ep(), _idx()
    url = f"{ep}/indexes('{idx}')/docs/search?api-version={API_VERSION}"
    body = {
      "count": True,
      "select": "id,originalId,name,source,path,lastModified",
      "vectorQueries": [
        {"kind":"vector", "vector": qvec, "exhaustive": False, "k": k, "fields": "contentVector"}
      ]
    }
    r = requests.post(url, headers=_hdr(), json=body, timeout=60)
    if r.status_code >= 400:
        try: st.error(f"[vector_search] {r.status_code} {r.text}")
        except Exception: pass
        r.raise_for_status()
    return r.json()

def show_search_guidance(st_container=None):
    """
    Search ì¸ë±ìŠ¤ ë° ë²¡í„° ê²€ìƒ‰ ê°€ì´ë“œ í‘œì‹œìš© (Streamlit UI)
    """
    c = st_container or st
    c.markdown("### ğŸ” Cognitive Search ê°€ì´ë“œ")
    c.markdown("""
    **DocSpace AI**ëŠ” Azure Cognitive Searchë¥¼ ì´ìš©í•´ ë¬¸ì„œë¥¼ ì¸ë±ì‹±í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.  

    **ì£¼ìš” í•¨ìˆ˜**
    - `create_index_if_missing()` : ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸ ë° ìë™ ìƒì„±
    - `upsert_documents(docs)` : ë¬¸ì„œ ì¸ë±ìŠ¤ì— ì—…ì„œíŠ¸
    - `vector_search(query_text, k)` : ë²¡í„°/í…ìŠ¤íŠ¸ ê²€ìƒ‰

    ì¸ë±ìŠ¤ í•„ë“œ ì˜ˆì‹œ:
    ```
    id (key) | name | content | lastModified | views
    ```
    """)

def get_document_by_id(doc_id: str) -> dict:
    """
    ì¸ë±ìŠ¤ì—ì„œ íŠ¹ì • idì˜ ë¬¸ì„œ ë‹¨ê±´ ì¡°íšŒ (content í¬í•¨)
    """
    ep = CONFIG["SEARCH_ENDPOINT"].rstrip("/")
    idx = CONFIG["SEARCH_INDEX"]
    url = f"{ep}/indexes('{idx}')/docs?api-version={API_VERSION}&$filter=id eq '{doc_id}'&$top=1"
    r = requests.get(url, headers=_hdr(), timeout=30)
    r.raise_for_status()
    vals = r.json().get("value", [])
    return vals[0] if vals else {}

def vector_search_by_text(text: str, k: int = 5, select: str = "id,name,lastModified,views") -> List[Dict]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì¿¼ë¦¬í•´ ìƒìœ„ kê°œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ë°˜í™˜
    (ë²¡í„°/í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì„± ì „ PoCìš© simple search)
    """
    ep = CONFIG["SEARCH_ENDPOINT"].rstrip("/")
    idx = CONFIG["SEARCH_INDEX"]
    url = f"{ep}/indexes('{idx}')/docs/search?api-version={API_VERSION}"
    payload = {"search": text[:3000], "queryType": "simple", "top": k, "select": select}
    r = requests.post(url, headers=_hdr(), json=payload, timeout=30)
    r.raise_for_status()
    vals = r.json().get("value", [])
    # ì •ê·œí™”
    out = []
    for v in vals:
        out.append({
            "id": v.get("id"),
            "name": v.get("name"),
            "lastModified": v.get("lastModified"),
            "views": v.get("views"),
            "score": v.get("@search.score"),
        })
    return out
